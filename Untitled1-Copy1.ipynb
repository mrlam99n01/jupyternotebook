{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch vision pakage comprises \n",
    "datasets (like MNIST and Fashion-MNIST)\n",
    "models\n",
    "transfoms\n",
    "utilis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#extract - get the fashion-MNIST from the source\n",
    "\n",
    "#transform - put our data in to tensor form\n",
    "\n",
    "#load- put our data into an object to make it easy accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST'\n",
    "    ,train = True\n",
    "    ,download = True\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size = 100\n",
    ")\n",
    "#train_loader is an object so that it isn't iterable\n",
    "# so i have to put iter to make sure that it is iteratable \n",
    "batch = next(iter(train_loader))\n",
    "#type of batch is a list after next\n",
    "mini_batch, classes = batch\n",
    "#mini_batch va classes are torch \n",
    "print(mini_batch.shape)\n",
    "print(classes.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels =1, out_channels = 6, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4, out_features = 120)\n",
    "        self.fc2 = nn.Linear(in_features = 120, out_features = 60)\n",
    "        self.out = nn.Linear(in_features = 60, out_features = 10)\n",
    "    def forward(self,t):\n",
    "        t = F.relu(self.conv1(t))\n",
    "        t = F.max_pool2d(t,kernel_size =2, stride=2)\n",
    "        t = F.relu(self.conv2(t))\n",
    "        t = F.max_pool2d(t, kernel_size =2, stride=2)\n",
    "        t = F.relu(self.fc1(t.reshape(-1,12*4*4)))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0887,  0.0416, -0.0739, -0.0993,  0.0659,  0.0812,  0.0584,  0.0790,\n",
      "          0.0961,  0.0015]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "image,label =  train_set[0]\n",
    "image.shape\n",
    "image.unsqueeze(0).shape\n",
    "pred = network(image.unsqueeze(0))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1054, 0.1006, 0.0896, 0.0873, 0.1030, 0.1046, 0.1023, 0.1044, 0.1062,\n",
       "         0.0966]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(pred,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3140761852264404"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds,labels)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    HOW TO TRAIN\n",
    "    Get batch from the training set.\n",
    "    Pass batch to network.\n",
    "    Calculate the loss (difference between the predicted values and the true values).\n",
    "    Calculate the gradient of the loss function w.r.t the network's weights.\n",
    "    Update the weights using the gradients to reduce the loss.\n",
    "    Repeat steps 1-5 until one epoch is completed.\n",
    "    Repeat steps 1-6 for as many epochs required to reach the minimum loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f8b7d9557d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3141, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "preds = network(images)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# shape \n",
    "print(network.conv1.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward(retain_graph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1:  2.3140761852264404\n",
      "loss2:  2.282193422317505\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(network.parameters(),lr=0.01)\n",
    "# remerber network.parameters is network.weighs\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "preds = network(images) #pass batch\n",
    "loss = F.cross_entropy(preds,labels)\n",
    "loss.backward()\n",
    "optimizer.step() #update weights\n",
    "print('loss1: ',loss.item())\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds,labels)\n",
    "print('loss2: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t\t torch.Size([6, 1, 5, 5])\n",
      "conv1.bias \t\t torch.Size([6])\n",
      "conv2.weight \t\t torch.Size([12, 6, 5, 5])\n",
      "conv2.bias \t\t torch.Size([12])\n",
      "fc1.weight \t\t torch.Size([120, 192])\n",
      "fc1.bias \t\t torch.Size([120])\n",
      "fc2.weight \t\t torch.Size([60, 120])\n",
      "fc2.bias \t\t torch.Size([60])\n",
      "out.weight \t\t torch.Size([10, 60])\n",
      "out.bias \t\t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,param in network.named_parameters():\n",
    "    print(name,'\\t\\t',param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return torch.sum(torch.eq(torch.argmax(preds,dim=1),labels)).item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eporch:  0 total_correct:  47082 loss:  338.89317589998245\n",
      "eporch:  1 total_correct:  51164 loss:  235.74559946358204\n",
      "eporch:  2 total_correct:  51814 loss:  220.47338744997978\n",
      "eporch:  3 total_correct:  52162 loss:  210.86945602297783\n",
      "eporch:  4 total_correct:  52518 loss:  202.01242849230766\n"
     ]
    }
   ],
   "source": [
    "# train through eporch\n",
    "network = Network()\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size =100)\n",
    "optimizer = optim.Adam(network.parameters(),lr =0.01)\n",
    "\n",
    "for eporch in range(5):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "        preds = network(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(preds,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds,labels)\n",
    "    print(\"eporch: \", eporch, \"total_correct: \", total_correct,\"loss: \", total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8753"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_correct/len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat((all_preds,preds),dim =0)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0820e+01, -1.9960e+01, -1.7543e+01,  ...,  8.0605e+00, -1.6426e+01,  1.6161e+01],\n",
       "        [ 1.1819e+01, -7.9368e+00, -1.4245e+00,  ..., -2.6334e+01, -5.3675e+00, -1.5202e+01],\n",
       "        [ 3.1602e+00, -5.7749e-01, -2.2274e+00,  ..., -1.3129e+01, -3.4554e+00, -7.4124e+00],\n",
       "        ...,\n",
       "        [ 5.6966e-01,  4.5526e-02, -3.6531e+00,  ..., -2.7059e+01, -1.0088e+01, -1.9243e+01],\n",
       "        [ 5.7482e+00, -4.2974e+00,  6.7863e-03,  ..., -1.2566e+01, -2.0333e+00, -7.8471e+00],\n",
       "        [-3.5887e+00, -1.2466e+01, -1.0457e+01,  ...,  3.0499e+00, -1.5907e+00, -7.6852e-01]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_preds(network,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
